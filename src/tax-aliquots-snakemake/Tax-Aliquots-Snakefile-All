configfile: "config.yaml"

import os
import pandas as pd
import numpy as np
import scipy
import random
import json
import sys
import time
import subprocess
import itertools
import dask
import difflib
#import pysais
import argparse, textwrap
#import parsl
from Bio import SeqIO
from dask import delayed
from dask.distributed import Client
import faSomeRecords as fasomerecords
from difflib import SequenceMatcher
from sklearn.cluster import AgglomerativeClustering
from sklearn.datasets import make_blobs
from scipy.cluster.hierarchy import linkage, fcluster, dendrogram
import ahocorasick
from ahocorasick import Automaton
#from parsl import python_app, bash_app

OUTPUTDIR = "../../output/tax-aliquots_sm_8_all_mem_cov_50"
DATABASEDIR = "/vortexfs1/omics/alexander/akrinos/00-tax-aliquots/tax-aliquots/data/databases_full"

ruleorder: filter_deepclust > diamond_deepclust
ruleorder: hierarchical_clustering_split > hierarchical_clustering
def select_file(wc_database):
    if wc_database=="thal":
        return os.path.join(DATABASEDIR,"subsetted_fasta_thalassiosirales.fasta")
    elif wc_database=="phaeo":
        return os.path.join(DATABASEDIR,"subsetted_fasta_phaeocystis.fasta")
    elif wc_database=="phaeo_txme":
        return os.path.join(DATABASEDIR,"subsetted_fasta_phaeocystis_free.fasta")
    elif wc_database=="phaeo_txme_all":
        return os.path.join(DATABASEDIR,"subsetted_fasta_phaeocystis.fasta")
    elif wc_database=="phaeo_txme_all_w_pouch":
        return os.path.join(DATABASEDIR,"subsetted_fasta_phaeocystis_w_pouch.fasta")
 
def select_tax_table(wc_database):
    if wc_database=="thal":
        return os.path.join(DATABASEDIR,"tax_table_thalassiosirales.txt")
    elif wc_database=="phaeo":
        return os.path.join(DATABASEDIR,"tax_table_phaeocystis.txt")
    elif wc_database=="phaeo_txme":
        return os.path.join(DATABASEDIR,"tax_table_phaeocystis_free.txt")
    elif wc_database=="phaeo_txme_all":
        return os.path.join(DATABASEDIR,"tax_table_phaeocystis.txt")
    elif wc_database=="phaeo_txme_all_w_pouch":
        return os.path.join(DATABASEDIR,"tax_table_phaeocystis_w_pouch.txt")
        
def select_data_file(wc_database):
    if wc_database=="thal":
        return os.path.join("/vortexfs1/omics/alexander/akrinos/2021-testing-eukrhythmic/eukrhythmic_paper_narragansett/intermediate-files/04-compare/13-MAD-proteins/MAD.fasta.transdecoder.pep")
    elif wc_database=="phaeo":
        return os.path.join("/vortexfs1/omics/alexander/data/TARA/PRJEB4352-snakmake-output/prodigal/SO-all-SRF-0.8-5.00/proteins.faa")
    elif wc_database=="phaeo_txme":
        return os.path.join(DATABASEDIR,"Pouch_Trinity_transdecoder_decontam.pep")
    elif wc_database=="phaeo_txme_all":
        return os.path.join(DATABASEDIR,"Pouch_Trinity_transdecoder_decontam.pep")
    elif wc_database=="phaeo_txme_all_w_pouch":
        return os.path.join(DATABASEDIR,"Pouch_Trinity_transdecoder_decontam.pep")
    

def read_in_sequences(behaved_fasta,list_of_seqs,intermed_file_dir):
    sequence_list = fasomerecords.main(behaved_fasta,list_of_seqs,os.path.join(intermed_file_dir,"outfile.fasta"),False)
    recs = [curr for curr in SeqIO.parse(os.path.join(intermed_file_dir,"outfile.fasta"),"fasta")]
    return recs

def compute_intersections(strings,kmer_len,cluster_seq_ids):
    interecting_strings=dict()
    len_intersec_strings=[]
    n=int(kmer_len)
    keywords = [string[i:i+n] for string in strings for i in range(len(string) - n + 1)]

    automaton = Automaton()
    for i, keyword in enumerate(keywords):
        automaton.add_word(keyword, (i, keyword))
    automaton.make_automaton()

    for i, string in enumerate(strings):
        interecting_strings[string] = set([curr[1][1] for curr in list(automaton.iter(string))])
        len_intersec_strings.append(len(interecting_strings[string]))
    intersection_info_curr = np.zeros((len(strings),len(strings)))
    
    
    for i,string1 in enumerate(strings):
        for j,string2 in enumerate(strings):
            min_length=min(len(interecting_strings[string1]),
                           len(interecting_strings[string2]))
            if min_length==0:
                min_length=1
            intersection_info_curr[i,j] = (min_length-len(interecting_strings[string1].\
                                          intersection(interecting_strings[string2])))/min_length


    intersection_info_curr  = np.triu(intersection_info_curr) + np.triu(intersection_info_curr,1).T
    intersection_info_curr = pd.DataFrame(intersection_info_curr)
    print(cluster_seq_ids,flush=True)
    intersection_info_curr.index=cluster_seq_ids
    intersection_info_curr.columns=cluster_seq_ids
    return intersection_info_curr

def get_list_of_seqs(behaved_fasta,list_of_seqs,intermed_file_dir):
    #print(read_in_sequences(behaved_fasta,list_of_seqs,intermed_file_dir),flush=True)
    #print("READINSEQS",flush=True)
    return [str(curr.seq) for curr in read_in_sequences(behaved_fasta,list_of_seqs,intermed_file_dir)]

def hier_clust(clust_rep,subfile_clust,kmer_len,
               intermed_file_dir,hom_clust_id,behaved_fasta,
               tax_table,distance_threshold):
    subfile_clust=subfile_clust.drop_duplicates()
    strings=get_list_of_seqs(behaved_fasta,list(subfile_clust.loc[subfile_clust.Rep==clust_rep,"Mem"]),intermed_file_dir)
    if len(strings) == 0:
        print(clust_rep,"not found",flush=True)
        return hom_clust_id
    cluster_seq_ids=list(subfile_clust.loc[subfile_clust.Rep==clust_rep,"Mem"])
    
    if len(strings) != len(cluster_seq_ids):
        print(clust_rep,"length_mismatch",flush=True)
        print(cluster_seq_ids,flush=True)
        print(strings,flush=True)
        return hom_clust_id
    hier_clust_input=compute_intersections(strings,kmer_len,cluster_seq_ids)
    hier_clust_input["Source_ID"]=[curr.split("SOURCEID_")[-1] for curr in hier_clust_input.index]
    hier_clust_input["PEP_ID"]=[curr.split("PEPID_")[-1] for curr in hier_clust_input.index]
    sequence_names = hier_clust_input.index
    
    hier_clust_input.index.rename('sequence', inplace=True)
    
    dataframe_intersections_unmelt= hier_clust_input.merge(subfile_clust,
                                                           left_on="sequence",
                                                           right_on="Mem")
    
    dataframe_intersections_unmelt = dataframe_intersections_unmelt.merge(tax_table.drop_duplicates(),
                                                                          left_on="Source_ID",
                                        right_on="source_id",how="left")

    if len(dataframe_intersections_unmelt.index) > 1:
        to_drop = list(tax_table.columns) + ["Source_ID","PEP_ID","Rep","Mem"]
        #to_drop = ["Source_ID","PEP_ID","Rep","Mem"]
        a=np.array(dataframe_intersections_unmelt.\
                   drop(to_drop,axis="columns"))
        np.fill_diagonal(a,0)
        print(a.shape,flush=True)
        if a.shape[0] != a.shape[1]:
            print(hier_clust_input,flush=True)
            print(hier_clust_input.columns,flush=True)
            print(dataframe_intersections_unmelt,flush=True)
            print(dataframe_intersections_unmelt.columns,flush=True)
            print(subfile_clust,flush=True)
        Z = linkage(scipy.spatial.distance.squareform(a),
                method='weighted')
        max_d=float(distance_threshold)
        clusters = fcluster(Z, max_d, criterion='distance')
        dataframe_intersections_unmelt["cluster_blob"] = clusters
    else:
        dataframe_intersections_unmelt["cluster_blob"] = 1

    dataframe_intersections_unmelt["HomClust"]=hom_clust_id.split("/")[-1].split(".")[0]
    dataframe_intersections_unmelt["SeqNames"] = sequence_names
    #dataframe_intersections_unmelt.loc[:,["Source_ID","PEP_ID","HomClust","cluster_blob","division","domain"]].\
    dataframe_intersections_unmelt.loc[:,["Source_ID","PEP_ID","HomClust","cluster_blob","division","domain"]].\
        to_csv(os.path.join(intermed_file_dir,str(hom_clust_id)+"_k_"+str(kmer_len)+".intermed"))
    dataframe_intersections_unmelt.\
        to_csv(os.path.join(intermed_file_dir,"withdist"+str(hom_clust_id)+"_k_"+str(kmer_len)+".intermed"))
    return hom_clust_id

split_no=50000
nseqs_phaeo=1000000
nseqs_thal=100000000
rule all:
    input:
        #outfile_sample_clust = expand(os.path.join(OUTPUTDIR,"sample_hier_clust_{dist}","done_{database}.txt"),
        #                              database=["phaeo_txme_all"],
        #                              dist=[0.5,0.3,0.1,0.8]),
        outfile_sample_clust_default = expand(os.path.join(OUTPUTDIR,"REF_sample_hier_clust_{dist}","done_{database}.txt"),
                                      database=["phaeo_txme_all"],
                                      dist=[0.5,0.3,0.1,0.8]),  
        eukulele = expand(os.path.join(OUTPUTDIR,"eukulele_default","{database}","done_default.txt"),database=["phaeo_txme_all"]) 

rule combined_files:
    input:
        database_file = lambda wc: select_file(wc.database),
        sequence_file = lambda wc: select_data_file(wc.database)
    output:
        combined_file = os.path.join(OUTPUTDIR,"input_files","combined_input_{database}.fasta")
    shell:
        """
        echo sed -E 's/([^ ]*) (.*)\/SOURCE_ID=(.*)/\\1_reference_PEPID_\\1_SOURCEID_\\3/' {input.database_file} > {output.combined_file}
        sed -E 's/([^ ]*) (.*)\/SOURCE_ID=(.*)/\\1_reference_PEPID_\\1_SOURCEID_\\3/' {input.database_file} > {output.combined_file}
        sed 's/\s.*$/_sample/' {input.sequence_file} >> {output.combined_file}
        """

rule combined_files_ref:
    input:
        database_file = "/vortexfs1/omics/alexander/data/databases/marmmetsp-5Dec2022/reference.pep.fa",
        sequence_file = lambda wc: select_data_file(wc.database)
    output:
        combined_file = os.path.join(OUTPUTDIR,"REF_input_files","combined_input_{database}.fasta")
    shell:
        """
        echo sed -E 's/([^ ]*) (.*)\/SOURCE_ID=(.*)/\\1_reference_PEPID_\\1_SOURCEID_\\3/' {input.database_file} > {output.combined_file}
        sed -E 's/([^ ]*) (.*)\/SOURCE_ID=(.*)/\\1_reference_PEPID_\\1_SOURCEID_\\3/' {input.database_file} > {output.combined_file}
        sed 's/\s.*$/_sample/' {input.sequence_file} >> {output.combined_file}
        """

#sed 's/\s.*$/_reference/' {input.database_file} > {output.combined_file}
#sed 's/\s.*$/_sample/' {input.sequence_file} >> {output.combined_file}
rule diamond_deepclust_ref:
    input:
        combined_file = os.path.join(OUTPUTDIR,"REF_input_files","combined_input_{database}.fasta")
    output:
        deepclust = os.path.join(OUTPUTDIR,"REF_clustered_files","deepclust_{database}.out")
    shell:
        """
        /vortexfs1/omics/alexander/akrinos/2022-euk-diversity/code/snakemake-workflows/diamond-deepclust/diamond deepclust -d {input.combined_file} -o {output.deepclust} --member-cover 50
        """
        
rule filter_deepclust_ref:
    input:
        deepclust = os.path.join(OUTPUTDIR,"REF_clustered_files","deepclust_{database}.out")
    output:
        deepclust_filtered = os.path.join(OUTPUTDIR,"REF_clustered_files_sample","deepclust_filtered_sample_{database}.out"),
        deepclust_filtered_rep_mem = os.path.join(OUTPUTDIR,"REF_clustered_files_sample","repmem_deepclust_filtered_sample_{database}.out")
    run:
        deepclust=pd.read_csv(input.deepclust,sep="\t",header=None,names=["Representative","Member"])
        deepclust_forcheck=deepclust.copy(deep=True)
        deepclust_forcheck["Member_simp"]=["database" if "_reference" in curr else "sample" for curr in deepclust_forcheck["Member"]]
        def listset(list_in):
            return "_".join(sorted(list(set(list_in))))
        deepclust_summarized = deepclust_forcheck.groupby("Representative")["Member_simp"].agg(listset).reset_index()
        deepclust_summarized=deepclust_summarized.loc[(deepclust_summarized.Member_simp=="database_sample")|(deepclust_summarized.Member_simp=="sample"),:]
        
        merged_dc = deepclust.merge(deepclust_summarized[["Representative","Member_simp"]],left_on="Representative",
                right_on="Representative",how="inner")
                
        merged_dc[["Member"]].to_csv(output.deepclust_filtered,index=False)
        merged_dc[["Representative","Member"]].to_csv(output.deepclust_filtered_rep_mem,index=False,sep="\t")

rule filter_assembly_samp_ref:
    input:
        combined_file = os.path.join(OUTPUTDIR,"REF_input_files","combined_input_{database}.fasta"),
        deepclust_filtered = os.path.join(OUTPUTDIR,"REF_clustered_files_sample","deepclust_filtered_sample_{database}.out")
    output:
        deepclust_filtered = os.path.join(OUTPUTDIR,"REF_filtered_assembly_sample","filtered_assembly_sample_{database}.fasta")
    shell:
        """
        seqtk subseq {input.combined_file} {input.deepclust_filtered} > {output.deepclust_filtered}
        """
        

#sed 's/\s.*$/_reference/' {input.database_file} > {output.combined_file}
#sed 's/\s.*$/_sample/' {input.sequence_file} >> {output.combined_file}
rule diamond_deepclust:
    input:
        combined_file = os.path.join(OUTPUTDIR,"input_files","combined_input_{database}.fasta")
    output:
        deepclust = os.path.join(OUTPUTDIR,"clustered_files","deepclust_{database}.out")
    shell:
        """
        /vortexfs1/omics/alexander/akrinos/2022-euk-diversity/code/snakemake-workflows/diamond-deepclust/diamond deepclust -d {input.combined_file} -o {output.deepclust} --member-cover 80
        """

rule filter_deepclust:
    input:
        deepclust = os.path.join(OUTPUTDIR,"clustered_files","deepclust_{database}.out")
    output:
        deepclust_filtered = os.path.join(OUTPUTDIR,"clustered_files","deepclust_filtered_{database}.out"),
        deepclust_filtered_rep_mem = os.path.join(OUTPUTDIR,"clustered_files","repmem_deepclust_filtered_{database}.out")
    run:
        deepclust=pd.read_csv(input.deepclust,sep="\t",header=None,names=["Representative","Member"])
        deepclust_forcheck=deepclust.copy(deep=True)
        deepclust_forcheck["Member_simp"]=["database" if "_reference" in curr else "sample" for curr in deepclust_forcheck["Member"]]
        def listset(list_in):
            return "_".join(sorted(list(set(list_in))))
        deepclust_summarized = deepclust_forcheck.groupby("Representative")["Member_simp"].agg(listset).reset_index()
        deepclust_summarized=deepclust_summarized.loc[deepclust_summarized.Member_simp=="database_sample",:]
        
        merged_dc = deepclust.merge(deepclust_summarized[["Representative","Member_simp"]],left_on="Representative",
                right_on="Representative",how="inner")
                
        merged_dc[["Member"]].to_csv(output.deepclust_filtered,index=False)
        merged_dc[["Representative","Member"]].to_csv(output.deepclust_filtered_rep_mem,index=False,sep="\t")


rule eukulele_run_def:
    input:
        sequence_file = lambda wc: select_data_file(wc.database)
    output:
        finished = os.path.join(OUTPUTDIR,"eukulele_default","{database}","done_default.txt")
    params:
        sampledir = os.path.join(DATABASEDIR,"sample-dir"),
        eukulele_reference_dir = "/vortexfs1/omics/alexander/data/databases/marmmetsp-5Dec2022/",
        eukulele_directory = os.path.join(OUTPUTDIR,"eukulele_default","{database}")
    shell:
        """
        EUKulele --mets_or_mags mets --sample_dir {params.sampledir} --p_ext ".pep" --reference_dir {params.eukulele_reference_dir} -o {params.eukulele_directory}
        touch {output.finished}
        """
        
rule eukulele_run:
    input:
        sequence_file = lambda wc: select_data_file(wc.database)
    output:
        finished = os.path.join(OUTPUTDIR,"eukulele_run","{database}","done.txt")
    params:
        sampledir = os.path.join(DATABASEDIR,"sample-dir"),
        eukulele_reference_dir = os.path.join(DATABASEDIR,"{database}"),
        eukulele_directory = os.path.join(OUTPUTDIR,"eukulele_run","{database}")
    shell:
        """
        EUKulele --mets_or_mags mets --sample_dir {params.sampledir} --p_ext ".pep" --reference_dir {params.eukulele_reference_dir} -o {params.eukulele_directory}
        touch {output.finished}
        """

rule filter_assembly:
    input:
        combined_file = os.path.join(OUTPUTDIR,"input_files","combined_input_{database}.fasta"),
        deepclust_filtered = os.path.join(OUTPUTDIR,"clustered_files","deepclust_filtered_{database}.out")
    output:
        deepclust_filtered = os.path.join(OUTPUTDIR,"filtered_assembly","filtered_assembly_{database}.fasta")
    shell:
        """
        seqtk subseq {input.combined_file} {input.deepclust_filtered} > {output.deepclust_filtered}
        """
        
rule filter_deepclust_sample:
    input:
        deepclust = os.path.join(OUTPUTDIR,"clustered_files","deepclust_{database}.out")
    output:
        deepclust_filtered = os.path.join(OUTPUTDIR,"clustered_files_sample","deepclust_filtered_sample_{database}.out"),
        deepclust_filtered_rep_mem = os.path.join(OUTPUTDIR,"clustered_files_sample","repmem_deepclust_filtered_sample_{database}.out")
    run:
        deepclust=pd.read_csv(input.deepclust,sep="\t",header=None,names=["Representative","Member"])
        deepclust_forcheck=deepclust.copy(deep=True)
        deepclust_forcheck["Member_simp"]=["database" if "_reference" in curr else "sample" for curr in deepclust_forcheck["Member"]]
        def listset(list_in):
            return "_".join(sorted(list(set(list_in))))
        deepclust_summarized = deepclust_forcheck.groupby("Representative")["Member_simp"].agg(listset).reset_index()
        deepclust_summarized=deepclust_summarized.loc[(deepclust_summarized.Member_simp=="database_sample")|(deepclust_summarized.Member_simp=="sample"),:]
        
        merged_dc = deepclust.merge(deepclust_summarized[["Representative","Member_simp"]],left_on="Representative",
                right_on="Representative",how="inner")
                
        merged_dc[["Member"]].to_csv(output.deepclust_filtered,index=False)
        merged_dc[["Representative","Member"]].to_csv(output.deepclust_filtered_rep_mem,index=False,sep="\t")

rule filter_assembly_samp:
    input:
        combined_file = os.path.join(OUTPUTDIR,"input_files","combined_input_{database}.fasta"),
        deepclust_filtered = os.path.join(OUTPUTDIR,"clustered_files_sample","deepclust_filtered_sample_{database}.out")
    output:
        deepclust_filtered = os.path.join(OUTPUTDIR,"filtered_assembly_sample","filtered_assembly_sample_{database}.fasta")
    shell:
        """
        seqtk subseq {input.combined_file} {input.deepclust_filtered} > {output.deepclust_filtered}
        """
        
rule hierarchical_clustering_sample_ref:
    input:
        deepclust_filtered = os.path.join(OUTPUTDIR,"REF_filtered_assembly_sample","filtered_assembly_sample_{database}.fasta"),
        deepclust_filtered_rep_mem = os.path.join(OUTPUTDIR,"REF_clustered_files_sample","repmem_deepclust_filtered_sample_{database}.out"),
        tax_table = lambda wc: select_tax_table(wc.database)
    output:
        outfile = os.path.join(OUTPUTDIR,"REF_sample_hier_clust_{distance}","done_{database}.txt")
    params:
        intermed_dir = os.path.join(OUTPUTDIR,"REF_sample_hier_clust_{distance}","{database}"),
        distance_thres = "{distance}"
    run:
        if not os.path.exists(params.intermed_dir):
            os.makedirs(params.intermed_dir)
        tax_table = pd.read_csv(input.tax_table,sep="\t")
        tax_table=tax_table.drop([curr for curr in tax_table.columns if "Unnamed" in curr],
                                 axis="columns")
        tax_table.columns = [curr.lower() for curr in tax_table.columns]
        tax_table.source_id = tax_table.source_id.astype(str)
        read_in_dc = pd.read_csv(input.deepclust_filtered_rep_mem,sep="\t",header=None,names=["Rep","Mem"])
        clust_rep_set=list(set(read_in_dc.Rep))
        cluster_ids = ["HomClust_"+str(curr) for curr in range(len(clust_rep_set))]
        cluster_id_dict = dict(zip(clust_rep_set,cluster_ids))

        intermed_file_dir=params.intermed_dir
        completion_nums = []
        kmer_len=3
        for clust_rep in clust_rep_set:
            subfile_clust=read_in_dc.loc[read_in_dc.Rep==clust_rep,:]
            if len(subfile_clust.index)<=1:
                subfile_clust["cluster_blob"]=1
                hom_clust_id=cluster_id_dict[clust_rep]
                subfile_clust["HomClust"]=hom_clust_id
                subfile_clust["Source_ID"]=[curr.split("SOURCEID_")[-1] for curr in subfile_clust.Rep]
                subfile_clust["PEP_ID"]=[curr.split("PEPID_")[-1] for curr in subfile_clust.Rep]
                subfile_clust.to_csv(os.path.join(intermed_file_dir,str(hom_clust_id)+"_k_"+str(kmer_len)+".intermed"))
                print(subfile_clust,"short!",flush=True)
            else:
                hom_clust_id=cluster_id_dict[clust_rep]
                completion_nums.append(hier_clust(clust_rep,subfile_clust,kmer_len,
                                                  intermed_file_dir,hom_clust_id,
                                                  input.deepclust_filtered,tax_table,params.distance_thres))

        # Main function: wait for all apps to finish and collect the results
        #outputs = [i.result() for i in completion_nums]
        
        tax_table.to_csv(output.outfile)
        
rule hierarchical_clustering:
    input:
        deepclust_filtered = os.path.join(OUTPUTDIR,"filtered_assembly","filtered_assembly_{database}.fasta"),
        deepclust_filtered_rep_mem = os.path.join(OUTPUTDIR,"clustered_files","repmem_deepclust_filtered_{database}.out"),
        deepclust_list = os.path.join(OUTPUTDIR,"clustered_files","deepclust_filtered_{database}.out"),
        tax_table = lambda wc: select_tax_table(wc.database)
    output:
        outfile = os.path.join(OUTPUTDIR,"hier_clust_{distance}","done_{database}.txt")
    params:
        intermed_dir = os.path.join(OUTPUTDIR,"hier_clust_{distance}","{database}"),
        distance_thres = "{distance}"
    run:
        if not os.path.exists(params.intermed_dir):
            os.makedirs(params.intermed_dir)
        tax_table = pd.read_csv(input.tax_table,sep="\t")
        tax_table=tax_table.drop([curr for curr in tax_table.columns if "Unnamed" in curr],
                                 axis="columns")
        tax_table.columns = [curr.lower() for curr in tax_table.columns]
        tax_table.source_id = tax_table.source_id.astype(str)
        read_in_dc = pd.read_csv(input.deepclust_filtered_rep_mem,sep="\t",header=None,names=["Rep","Mem"])
        clust_rep_set=list(set(read_in_dc.Rep))
        cluster_ids = ["HomClust_"+str(curr) for curr in range(len(clust_rep_set))]
        cluster_id_dict = dict(zip(clust_rep_set,cluster_ids))

        intermed_file_dir=params.intermed_dir
        completion_nums = []
        for clust_rep in clust_rep_set:
            subfile_clust=read_in_dc.loc[read_in_dc.Rep==clust_rep,:]
            if len(subfile_clust.index)<=1:
                print(subfile_clust,"short!",flush=True)
            else:
                hom_clust_id=cluster_id_dict[clust_rep]
                completion_nums.append(hier_clust(clust_rep,subfile_clust,3,
                                                  intermed_file_dir,hom_clust_id,
                                                  input.deepclust_filtered,tax_table,params.distance_thres))

        # Main function: wait for all apps to finish and collect the results
        #outputs = [i.result() for i in completion_nums]
        
        tax_table.to_csv(output.outfile)

rule hierarchical_clustering_all:
    input:
        deepclust_filtered = os.path.join(OUTPUTDIR,"input_files","combined_input_{database}.fasta"),
        deepclust_filtered_rep_mem = os.path.join(OUTPUTDIR,"clustered_files","deepclust_{database}.out"),
        tax_table = lambda wc: select_tax_table(wc.database)
    output:
        outfile = os.path.join(OUTPUTDIR,"all_hier_clust_{distance}","done_{database}.txt")
    params:
        intermed_dir = os.path.join(OUTPUTDIR,"all_hier_clust_{distance}","{database}"),
        distance_thres = "{distance}"
    run:
        if not os.path.exists(params.intermed_dir):
            os.makedirs(params.intermed_dir)
        tax_table = pd.read_csv(input.tax_table,sep="\t")
        tax_table=tax_table.drop([curr for curr in tax_table.columns if "Unnamed" in curr],
                                 axis="columns")
        tax_table.columns = [curr.lower() for curr in tax_table.columns]
        tax_table.source_id = tax_table.source_id.astype(str)
        read_in_dc = pd.read_csv(input.deepclust_filtered_rep_mem,sep="\t",header=None,names=["Rep","Mem"])
        clust_rep_set=list(set(read_in_dc.Rep))
        cluster_ids = ["HomClust_"+str(curr) for curr in range(len(clust_rep_set))]
        cluster_id_dict = dict(zip(clust_rep_set,cluster_ids))

        intermed_file_dir=params.intermed_dir
        completion_nums = []
        for clust_rep in clust_rep_set:
            subfile_clust=read_in_dc.loc[read_in_dc.Rep==clust_rep,:]
            if len(subfile_clust.index)<=1:
                print(subfile_clust,"short!",flush=True)
            else:
                hom_clust_id=cluster_id_dict[clust_rep]
                completion_nums.append(hier_clust(clust_rep,subfile_clust,3,
                                                  intermed_file_dir,hom_clust_id,
                                                  input.deepclust_filtered,tax_table,params.distance_thres))

        # Main function: wait for all apps to finish and collect the results
        #outputs = [i.result() for i in completion_nums]
        
        tax_table.to_csv(output.outfile)

rule hierarchical_clustering_sample:
    input:
        deepclust_filtered = os.path.join(OUTPUTDIR,"filtered_assembly_sample","filtered_assembly_sample_{database}.fasta"),
        deepclust_filtered_rep_mem = os.path.join(OUTPUTDIR,"clustered_files_sample","repmem_deepclust_filtered_sample_{database}.out"),
        tax_table = lambda wc: select_tax_table(wc.database)
    output:
        outfile = os.path.join(OUTPUTDIR,"sample_hier_clust_{distance}","done_{database}.txt")
    params:
        intermed_dir = os.path.join(OUTPUTDIR,"sample_hier_clust_{distance}","{database}"),
        distance_thres = "{distance}"
    run:
        if not os.path.exists(params.intermed_dir):
            os.makedirs(params.intermed_dir)
        tax_table = pd.read_csv(input.tax_table,sep="\t")
        tax_table=tax_table.drop([curr for curr in tax_table.columns if "Unnamed" in curr],
                                 axis="columns")
        tax_table.columns = [curr.lower() for curr in tax_table.columns]
        tax_table.source_id = tax_table.source_id.astype(str)
        read_in_dc = pd.read_csv(input.deepclust_filtered_rep_mem,sep="\t",header=None,names=["Rep","Mem"])
        clust_rep_set=list(set(read_in_dc.Rep))
        cluster_ids = ["HomClust_"+str(curr) for curr in range(len(clust_rep_set))]
        cluster_id_dict = dict(zip(clust_rep_set,cluster_ids))

        intermed_file_dir=params.intermed_dir
        completion_nums = []
        for clust_rep in clust_rep_set:
            subfile_clust=read_in_dc.loc[read_in_dc.Rep==clust_rep,:]
            if len(subfile_clust.index)<=1:
                print(subfile_clust,"short!",flush=True)
            else:
                hom_clust_id=cluster_id_dict[clust_rep]
                completion_nums.append(hier_clust(clust_rep,subfile_clust,3,
                                                  intermed_file_dir,hom_clust_id,
                                                  input.deepclust_filtered,tax_table,params.distance_thres))

        # Main function: wait for all apps to finish and collect the results
        #outputs = [i.result() for i in completion_nums]
        
        tax_table.to_csv(output.outfile)

rule split_files:
    input:
        deepclust_filtered_rep_mem = os.path.join(OUTPUTDIR,"clustered_files",
                        "repmem_deepclust_filtered_{database}.out")
    output:
        deepclust_filtered_rep_mem = os.path.join(OUTPUTDIR,"clustered_files",
                        "repmem_split_{split1}_deepclust_filtered_{database}.out")
    params:
        chunk_len=split_no,
        number="{split1}"
    shell:
        """
        head -n {params.number} {input.deepclust_filtered_rep_mem} | tail -n {params.chunk_len} > {output.deepclust_filtered_rep_mem}
        """

rule hierarchical_clustering_split:
    input:
        deepclust_filtered = os.path.join(OUTPUTDIR,"filtered_assembly","filtered_assembly_{database}.fasta"),
        deepclust_filtered_rep_mem = os.path.join(OUTPUTDIR,"clustered_files",
                        "repmem_split_{split1}_deepclust_filtered_{database}.out"),
        deepclust_list = os.path.join(OUTPUTDIR,"clustered_files","deepclust_filtered_{database}.out"),
        tax_table = lambda wc: select_tax_table(wc.database)
    output:
        outfile = os.path.join(OUTPUTDIR,"hier_clust_{distance}","{database}","{split1}",
            "done_{database}.txt")
    params:
        intermed_dir = os.path.join(OUTPUTDIR,"hier_clust_{distance}","{database}","{split1}"),
        distance_thres = "{distance}"
    run:
        if not os.path.exists(params.intermed_dir):
            os.makedirs(params.intermed_dir)
        tax_table = pd.read_csv(input.tax_table,sep="\t")
        tax_table=tax_table.drop([curr for curr in tax_table.columns if "Unnamed" in curr],
                                 axis="columns")
        tax_table.columns = [curr.lower() for curr in tax_table.columns]
        tax_table.source_id = tax_table.source_id.astype(str)
        read_in_dc = pd.read_csv(input.deepclust_filtered_rep_mem,sep="\t",header=None,names=["Rep","Mem"])
        clust_rep_set=list(set(read_in_dc.Rep))
        cluster_ids = ["HomClust_"+str(curr) for curr in range(len(clust_rep_set))]
        cluster_id_dict = dict(zip(clust_rep_set,cluster_ids))

        intermed_file_dir=params.intermed_dir
        completion_nums = []
        for clust_rep in clust_rep_set:
            subfile_clust=read_in_dc.loc[read_in_dc.Rep==clust_rep,:]
            if len(subfile_clust.index)==0:
                next
            hom_clust_id=cluster_id_dict[clust_rep]
            completion_nums.append(hier_clust(clust_rep,subfile_clust,3,
                                              intermed_file_dir,hom_clust_id,
                                              input.deepclust_filtered,tax_table,params.distance_thres))

        # Main function: wait for all apps to finish and collect the results
        outputs = [i.result() for i in completion_nums]
        
        tax_table.to_csv(output.outfile)
