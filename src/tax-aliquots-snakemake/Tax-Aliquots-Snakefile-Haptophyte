configfile: "config.yaml"

import os
import pandas as pd
import numpy as np
import scipy
import random
import json
import sys
import time
import subprocess
import itertools
import dask
import difflib
import argparse, textwrap
from Bio import SeqIO
from dask import delayed
from dask.distributed import Client
import faSomeRecords as fasomerecords
from difflib import SequenceMatcher
from sklearn.cluster import AgglomerativeClustering
from sklearn.datasets import make_blobs
from scipy.cluster.hierarchy import linkage, fcluster, dendrogram
import ahocorasick
from ahocorasick import Automaton
import concurrent.futures
from functools import partial

OUTPUTDIR = "../../output/tax-aliquots_haptophyte_3"
DATABASEDIR = "../data/databases_full"

ruleorder: filter_deepclust_ref > diamond_deepclust_ref
    
def read_in_sequences(behaved_fasta,list_of_seqs,intermed_file_dir):
    sequence_list = fasomerecords.main(behaved_fasta,list_of_seqs,os.path.join(intermed_file_dir,"outfile.fasta"),False)
    recs = [curr for curr in SeqIO.parse(os.path.join(intermed_file_dir,"outfile.fasta"),"fasta")]
    return recs

def compute_intersections(strings,kmer_len,cluster_seq_ids):
    interecting_strings=dict()
    len_intersec_strings=[]
    n=int(kmer_len)
    keywords = [string[i:i+n] for string in strings for i in range(len(string) - n + 1)]

    automaton = Automaton()
    for i, keyword in enumerate(keywords):
        automaton.add_word(keyword, (i, keyword))
    automaton.make_automaton()

    for i, string in enumerate(strings):
        interecting_strings[string] = set([curr[1][1] for curr in list(automaton.iter(string))])
        len_intersec_strings.append(len(interecting_strings[string]))
    intersection_info_curr = np.zeros((len(strings),len(strings)))
    
    
    for i,string1 in enumerate(strings):
        for j,string2 in enumerate(strings):
            min_length=min(len(interecting_strings[string1]),
                           len(interecting_strings[string2]))
            if min_length==0:
                min_length=1
            intersection_info_curr[i,j] = (min_length-len(interecting_strings[string1].\
                                          intersection(interecting_strings[string2])))/min_length


    intersection_info_curr  = np.triu(intersection_info_curr) + np.triu(intersection_info_curr,1).T
    intersection_info_curr = pd.DataFrame(intersection_info_curr)
    intersection_info_curr.index=cluster_seq_ids
    intersection_info_curr.columns=cluster_seq_ids
    return intersection_info_curr

def get_list_of_seqs(behaved_fasta,list_of_seqs,intermed_file_dir):
    return [str(curr.seq) for curr in read_in_sequences(behaved_fasta,list_of_seqs,intermed_file_dir)]

def hier_clust(clust_rep,subfile_clust,kmer_len,
               intermed_file_dir,hom_clust_id,behaved_fasta,
               tax_table,distance_threshold):
    subfile_clust=subfile_clust.drop_duplicates()
    strings=get_list_of_seqs(behaved_fasta,list(subfile_clust.loc[subfile_clust.Rep==clust_rep,"Mem"]),intermed_file_dir)
    if len(strings) == 0:
        return hom_clust_id
    cluster_seq_ids=list(subfile_clust.loc[subfile_clust.Rep==clust_rep,"Mem"])
    
    if len(strings) != len(cluster_seq_ids):
        print(clust_rep,"length_mismatch",flush=True)
        print(cluster_seq_ids,flush=True)
        print(strings,flush=True)
        return hom_clust_id
    hier_clust_input=compute_intersections(strings,kmer_len,cluster_seq_ids)
    hier_clust_input["Source_ID"]=[curr.split("SOURCEID_")[-1] for curr in hier_clust_input.index]
    hier_clust_input["PEP_ID"]=[curr.split("PEPID_")[-1] for curr in hier_clust_input.index]
    sequence_names = hier_clust_input.index
    
    hier_clust_input.index.rename('sequence', inplace=True)
    
    dataframe_intersections_unmelt= hier_clust_input.merge(subfile_clust,
                                                           left_on="sequence",
                                                           right_on="Mem")
    
    dataframe_intersections_unmelt = dataframe_intersections_unmelt.merge(tax_table.drop_duplicates(),
                                                                          left_on="Source_ID",
                                        right_on="source_id",how="left")

    if len(dataframe_intersections_unmelt.index) > 1:
        to_drop = list(tax_table.columns) + ["Source_ID","PEP_ID","Rep","Mem"]
        a=np.array(dataframe_intersections_unmelt.\
                   drop(to_drop,axis="columns"))
        np.fill_diagonal(a,0)
        Z = linkage(scipy.spatial.distance.squareform(a),
                method='weighted')
        max_d=float(distance_threshold)
        clusters = fcluster(Z, max_d, criterion='distance')
        dataframe_intersections_unmelt["cluster_blob"] = clusters
    else:
        dataframe_intersections_unmelt["cluster_blob"] = 1
    print(hom_clust_id,"hier",flush=True)
    dataframe_intersections_unmelt["HomClust"]=hom_clust_id.split("/")[-1].split(".")[0]
    dataframe_intersections_unmelt["SeqNames"] = sequence_names
    dataframe_intersections_unmelt.loc[:,["Source_ID","PEP_ID","HomClust","cluster_blob","division","domain"]].\
        to_csv(os.path.join(intermed_file_dir,str(hom_clust_id)+"_k_"+str(kmer_len)+".intermed"))
    dataframe_intersections_unmelt.\
        to_csv(os.path.join(intermed_file_dir,"withdist"+str(hom_clust_id)+"_k_"+str(kmer_len)+".intermed"))
    return hom_clust_id


def submit_routine(clust_rep,read_in_dc,kmer_len,cluster_id_dict,intermed_file_dir,
                                   deepclust_filtered,distance_thres,tax_table):
    subfile_clust=read_in_dc.loc[read_in_dc.Rep==clust_rep,:]
    if len(subfile_clust.index)<=1:
        subfile_clust["cluster_blob"]=1
        hom_clust_id=cluster_id_dict[clust_rep]
        subfile_clust["HomClust"]=hom_clust_id
        subfile_clust["Source_ID"]=[curr.split("SOURCEID_")[-1] for curr in subfile_clust.Rep]
        subfile_clust["PEP_ID"]=[curr.split("PEPID_")[-1] for curr in subfile_clust.Rep]
        subfile_clust.to_csv(os.path.join(intermed_file_dir,str(hom_clust_id)+"_k_"+str(kmer_len)+".intermed"))
    else:
        hom_clust_id=cluster_id_dict[clust_rep]
        hier_clust(clust_rep,subfile_clust,kmer_len,
                  intermed_file_dir,hom_clust_id,
                  deepclust_filtered,tax_table,distance_thres)

def parallelize_code(clust_rep_set,read_in_dc,kmer_len,cluster_id_dict,intermed_file_dir,
                     deepclust_filtered,distance_thres,tax_table):
    results=[]
    with concurrent.futures.ThreadPoolExecutor(max_workers=6) as executor:
        # Submit each row of the DataFrame for processing
        futures = [executor.submit(submit_routine,row,read_in_dc,kmer_len,cluster_id_dict,intermed_file_dir,
                                   deepclust_filtered,distance_thres,tax_table) for row in clust_rep_set]

    # Get the results as they become available
    for future in concurrent.futures.as_completed(futures):
        result = future.result()
        results.append(result)
        
rule all:
    input:
        outfile_sample_clust = expand(os.path.join(OUTPUTDIR,"sample_hier_clust_{dist}","cov_{coverage}",
                                                   "done_{coverage}.txt"),
                                          coverage=[30,50,80],
                                          dist=[0.5,0.3,0.1,0.8]),
        eukulele_out = os.path.join(OUTPUTDIR,"eukulele_run","haptophyte","done.txt")
        
rule combined_files:
    input:
        database_file = os.path.join(DATABASEDIR,"subsetted_fasta_haptophyta.fasta"),
        sequence_file = os.path.join(DATABASEDIR,"Pouch_Trinity_transdecoder_decontam.pep")
    output:
        combined_file = os.path.join(OUTPUTDIR,"input_files","combined_input.fasta")
    shell:
        """
        sed -E 's/([^ ]*) (.*)\/SOURCE_ID=(.*)/\\1_reference_PEPID_\\1_SOURCEID_\\3/' {input.database_file} > {output.combined_file}
        sed 's/\s.*$/_sample/' {input.sequence_file} >> {output.combined_file}
        """
        
rule diamond_deepclust_ref:
    input:
        combined_file = os.path.join(OUTPUTDIR,"input_files","combined_input.fasta")
    output:
        deepclust = os.path.join(OUTPUTDIR,"clustered_files","deepclust_{coverage}.out")
    params:
        coverage = "{coverage}"
    shell:
        """
        diamond deepclust -d {input.combined_file} -o {output.deepclust} --member-cover {params.coverage}
        """
        
rule filter_deepclust_ref:
    input:
        deepclust = os.path.join(OUTPUTDIR,"clustered_files","deepclust_{coverage}.out")
    output:
        deepclust_filtered = os.path.join(OUTPUTDIR,"clustered_files_sample",
            "deepclust_filtered_sample_{coverage}.out"),
        deepclust_filtered_rep_mem = os.path.join(OUTPUTDIR,"clustered_files_sample",
            "repmem_deepclust_filtered_sample_{coverage}.out")
    run:
        deepclust=pd.read_csv(input.deepclust,sep="\t",header=None,names=["Representative","Member"])
        deepclust_forcheck=deepclust.copy(deep=True)
        deepclust_forcheck["Member_simp"]=["database" if "_reference" in curr else \
            "sample" for curr in deepclust_forcheck["Member"]]
        def listset(list_in):
            return "_".join(sorted(list(set(list_in))))
        deepclust_summarized = deepclust_forcheck.groupby("Representative")["Member_simp"].agg(listset).reset_index()
        
        deepclust_summarized=deepclust_summarized.loc[(deepclust_summarized.Member_simp=="database_sample"),:]
        
        merged_dc = deepclust.merge(deepclust_summarized[["Representative","Member_simp"]],left_on="Representative",
                right_on="Representative",how="inner")
                
        merged_dc[["Member"]].to_csv(output.deepclust_filtered,index=False)
        merged_dc[["Representative","Member"]].to_csv(output.deepclust_filtered_rep_mem,index=False,sep="\t")
        
rule filter_assembly_samp_ref:
    input:
        combined_file = os.path.join(OUTPUTDIR,"input_files","combined_input.fasta"),
        deepclust_filtered = os.path.join(OUTPUTDIR,"clustered_files_sample","deepclust_filtered_sample_{coverage}.out")
    output:
        deepclust_filtered = os.path.join(OUTPUTDIR,"filtered_assembly_sample","filtered_assembly_sample_{coverage}.fasta")
    shell:
        """
        seqtk subseq {input.combined_file} {input.deepclust_filtered} > {output.deepclust_filtered}
        """
        
rule hierarchical_clustering_sample_ref:
    input:
        deepclust_filtered = os.path.join(OUTPUTDIR,"filtered_assembly_sample","filtered_assembly_sample_{coverage}.fasta"),
        deepclust_filtered_rep_mem = os.path.join(OUTPUTDIR,"clustered_files_sample",
            "repmem_deepclust_filtered_sample_{coverage}.out"),
        tax_table = os.path.join(DATABASEDIR,"tax_table_haptophyta.txt")
    output:
        outfile = os.path.join(OUTPUTDIR,"sample_hier_clust_{distance}","cov_{coverage}","done_{coverage}.txt")
    params:
        intermed_dir = os.path.join(OUTPUTDIR,"sample_hier_clust_{distance}","cov_{coverage}"),
        distance_thres = "{distance}"
    threads: 6
    run:
        if not os.path.exists(params.intermed_dir):
            os.makedirs(params.intermed_dir)
        tax_table = pd.read_csv(input.tax_table,sep="\t")
        tax_table=tax_table.drop([curr for curr in tax_table.columns if "Unnamed" in curr],
                                 axis="columns")
        tax_table.columns = [curr.lower() for curr in tax_table.columns]
        tax_table.source_id = tax_table.source_id.astype(str)
        read_in_dc = pd.read_csv(input.deepclust_filtered_rep_mem,sep="\t",header=None,names=["Rep","Mem"])
        clust_rep_set=list(set(read_in_dc.Rep))
        cluster_ids = ["HomClust_"+str(curr) for curr in range(len(clust_rep_set))]
        cluster_id_dict = dict(zip(clust_rep_set,cluster_ids))

        intermed_file_dir=params.intermed_dir
        kmer_len=3
                             
        parallelize_code(clust_rep_set,read_in_dc,kmer_len,cluster_id_dict,intermed_file_dir,
                         input.deepclust_filtered,params.distance_thres,tax_table)
        
        tax_table.to_csv(output.outfile)
        
rule eukulele_run:
    input:
        sequence_file = os.path.join(DATABASEDIR,"Pouch_Trinity_transdecoder_decontam.pep")
    output:
        finished = os.path.join(OUTPUTDIR,"eukulele_run","haptophyte","done.txt")
    params:
        sampledir = os.path.join(DATABASEDIR,"sample-dir"),
        eukulele_reference_dir = os.path.join(DATABASEDIR,"haptophyte_database"),
        eukulele_directory = os.path.join(OUTPUTDIR,"eukulele_run","haptophyte")
    shell:
        """
        EUKulele --mets_or_mags mets --sample_dir {params.sampledir} --p_ext ".pep" --reference_dir {params.eukulele_reference_dir} -o {params.eukulele_directory}
        touch {output.finished}
        """
